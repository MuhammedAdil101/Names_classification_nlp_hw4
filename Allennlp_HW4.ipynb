{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x193d85df6b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField,LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy , Average\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder,PytorchSeq2VecWrapper\n",
    "from torch.nn import LogSoftmax\n",
    "from torch.nn.modules import NLLLoss\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one word per line and its label \n",
    "    Doveyski , Russian.txt\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    def text_to_instance(self, tokens: List[Token], label: str =None) -> Instance:\n",
    "        word_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"word\": word_field}\n",
    " \n",
    "        if label is None:\n",
    "            return Instance(fields)\n",
    "        \n",
    "        \n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "            \n",
    "        return Instance(fields)\n",
    "       \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def findFiles(self,path): \n",
    "        return glob.glob(path)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "            \n",
    "        for filename in self.findFiles(file_path):\n",
    "            \n",
    "            with open(filename,encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    word= line.strip().split('\\n')\n",
    "                    word=str(word[0])\n",
    "                    yield self.text_to_instance([Token(ch) for ch in word], filename.split('\\\\')[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "3755it [00:00, 37276.39it/s]\n",
      "20074it [00:00, 25549.29it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = PosDatasetReader()\n",
    "train_dataset = reader.read('names/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([D, e, a, n, s], 'English.txt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3728].fields['word'].tokens , train_dataset[3728].fields['label'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 char_embeddings: TextFieldEmbedder,\n",
    "                 encoder:Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.char_embeddings = char_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = Average()\n",
    "        self.m = LogSoftmax()#I used that in order to escape from loss to blow up\n",
    "        self.loss = NLLLoss()#I used this from tutorial RNN classifier\n",
    "    def forward(self,\n",
    "                word: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        mask = get_text_field_mask(word)\n",
    "        embeddings = self.char_embeddings(word)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if label is not None:\n",
    "            output[\"loss\"] = self.loss(self.m(tag_logits), label)\n",
    "            pred = tag_logits.max(1)[1]#it is giving the maximum elements for each tensor inside pred and their indexes so we are taking indexes\n",
    "            self.accuracy((torch.eq(pred,label).sum()).double()/len(label))\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20074/20074 [00:00<00:00, 134153.89it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "char_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = WordClassifier(char_embeddings, lstm, vocab)\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"word\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10037 [00:00<?, ?it/s]C:\\Users\\Adil\\Anaconda2.1\\envs\\allennlp\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.5952, loss: 1.3568 ||: 100%|██████████| 10037/10037 [01:08<00:00, 146.01it/s]\n",
      "accuracy: 0.6800, loss: 1.0753 ||: 100%|██████████| 10037/10037 [01:06<00:00, 151.00it/s]\n",
      "accuracy: 0.6992, loss: 1.0142 ||: 100%|██████████| 10037/10037 [01:04<00:00, 156.26it/s]\n",
      "accuracy: 0.7043, loss: 0.9903 ||: 100%|██████████| 10037/10037 [01:06<00:00, 172.71it/s]\n",
      "accuracy: 0.7104, loss: 0.9707 ||: 100%|██████████| 10037/10037 [01:04<00:00, 154.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'peak_cpu_memory_MB': 0,\n",
       " 'training_duration': '00:05:31',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 4,\n",
       " 'epoch': 4,\n",
       " 'training_accuracy': tensor(0.7104, dtype=torch.float64),\n",
       " 'training_loss': 0.9706900416290403,\n",
       " 'training_cpu_memory_MB': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=5,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
